{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Module 05: Lab 02\"\n",
        "subtitle: \"Regression Modeling on Employment Data\"\n",
        "author:\n",
        "  - name: Pranjul Garg\n",
        "    affiliations:\n",
        "      - id: BU\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        " \n",
        "number-sections: true\n",
        "date: \"2025-04-14\"\n",
        "format:\n",
        "  html:\n",
        "    theme: cerulean\n",
        "    toc: true\n",
        "    toc-depth: 2\n",
        "date-modified: today\n",
        "date-format: long\n",
        "execute: \n",
        "  echo: false\n",
        "  eval: false\n",
        "  freeze: auto\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objectives {.unnumbered}\n",
        "\n",
        "1. Use **PySpark** to process the Lightcast dataset.\n",
        "2. Engineer features from structured columns for salary prediction.\n",
        "3. Train **Linear Regression model**.\n",
        "4. Evaluate models using **RMSE** and **R²**.\n",
        "5. Visualize predictions using diagnostic plots.\n",
        "6. Push work to GitHub and submit the repository link.\n",
        "\n",
        "# Setup {.unnumbered}\n",
        "\n",
        "The instruction below provides you with general keywords for columns used in the lightcast file. See the data schema generated after the load dataset code above to use proper column name. For visualizations, tables, or summaries, please **customize colors, fonts, and styles** as appropriate to avoid a **2.5-point deduction**. Also, **provide a two-sentence explanation** describing key insights drawn from each section's code and outputs. \n",
        "\n",
        "1. Follow the steps below as necessary, use your best judgement in importing/installing/creating/saving files as needed.\n",
        "2. Create a new Jupyter Notebook in your `ad688-sp25-lab08` directory named `lab08_yourname.ipynb`, if the file exists make sure to change the name.\n",
        "3. Use your **EC2 instance** for this lab.\n",
        "4. Ensure the `lightcast_data.csv` file is available on the EC2 instance. if not then **Download the dataset**\n",
        "5. **Add the dataset to `.gitignore`** to avoid pushing large files to GitHub. Open your `.gitignore` file and add:\n",
        "6. Make sure to create a virtual environment and install the required Python libraries if needed, don't forget to activate it:\n",
        "7. Install the required Python libraries if needed, you can also use the given requirement file to install the packages to the virtual environment:\n",
        "\n",
        "```bash\n",
        "python3 -m venv .venv\n",
        "source .venv/bin/activate\n",
        "gdown https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\n",
        "echo \"lightcast_job_postings.csv\" >> .gitignore\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import gdown\n",
        "#url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n",
        "#gdown.download(url, \"lightcast_data.csv\", quiet=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/04/14 11:23:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook\"\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "# Load Data\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_data.csv\")\n",
        "\n",
        "# Show Schema and Sample Data\n",
        "#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n",
        "\n",
        "#df.printSchema() # comment this line when rendering the submission\n",
        "#df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------+------+\n",
            "|features                       |SALARY|\n",
            "+-------------------------------+------+\n",
            "|[241000.0,144600.0,6.0,1.0,0.0]|192800|\n",
            "|[188600.0,63200.0,12.0,1.0,0.0]|125900|\n",
            "|[120328.0,116792.0,5.0,1.0,0.0]|118560|\n",
            "|[241000.0,144600.0,6.0,1.0,0.0]|192800|\n",
            "|[169800.0,63200.0,12.0,1.0,0.0]|116500|\n",
            "+-------------------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "#Drop rows with nulls in required columns\n",
        "df = df.dropna(subset=[\"SALARY\", \"SALARY_TO\", \"SALARY_FROM\", \"DURATION\", \"MIN_YEARS_EXPERIENCE\", \"EMPLOYMENT_TYPE_NAME\"])\n",
        "\n",
        "# Define columns\n",
        "categorical_cols = [\"EMPLOYMENT_TYPE_NAME\"]\n",
        "numerical_cols = [\"SALARY_TO\", \"SALARY_FROM\", \"MIN_YEARS_EXPERIENCE\"]\n",
        "\n",
        "#Index and encode categoricals\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid='skip')\n",
        "    for col in categorical_cols\n",
        "]\n",
        "\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_vec\", dropLast=True)\n",
        "    for col in categorical_cols\n",
        "]\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numerical_cols + [f\"{col}_vec\" for col in categorical_cols],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "\n",
        "pipeline_model = pipeline.fit(df)\n",
        "data = pipeline_model.transform(df)\n",
        "\n",
        "data.select(\"features\", \"SALARY\").show(5, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/14 11:23:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8716, 134)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5700, 134)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "train_data, test_data = data.randomSplit([0.6, 0.4], seed=801)  #U39153801\n",
        "print((train_data.count(), len(train_data.columns)))\n",
        "print((test_data.count(), len(test_data.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### I have used the general split type which is used while the spliting the data 60-40."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 16:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model Performance on Test Data ---\n",
            "R²:   0.9991\n",
            "RMSE: 1263.76\n",
            "MAE:  430.17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "feature_names = assembler.getInputCols()\n",
        "lr = LinearRegression(featuresCol='features', labelCol='SALARY', regParam=0.1)\n",
        "lr_model = lr.fit(train_data)\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_mae = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "\n",
        "r2 = evaluator_r2.evaluate(predictions)\n",
        "rmse = evaluator_rmse.evaluate(predictions)\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "\n",
        "print(\"\\n--- Model Performance on Test Data ---\")\n",
        "print(f\"R²:   {r2:.4f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE:  {mae:.2f}\")\n",
        "\n",
        "summary = lr_model.summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(flat_feature_names): 5\n",
            "len(coefficients): 5\n",
            "len(std_errs): 6\n",
            "len(tvals): 6\n",
            "len(pvals): 6\n"
          ]
        }
      ],
      "source": [
        "print(\"len(flat_feature_names):\", len(flat_feature_names))\n",
        "print(\"len(coefficients):\", len(list(lr_model.coefficients)))\n",
        "print(\"len(std_errs):\", len(list(summary.coefficientStandardErrors)))\n",
        "print(\"len(tvals):\", len(list(summary.tValues)))\n",
        "print(\"len(pvals):\", len(list(summary.pValues)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The length was mismatching, hence this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Confirmed corrected length: 5\n"
          ]
        }
      ],
      "source": [
        "metadata = data.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]\n",
        "\n",
        "# Flatten it safely into a list\n",
        "flat_feature_names = []\n",
        "for key in metadata:  # Usually 'numeric' and 'binary'\n",
        "    flat_feature_names += [attr[\"name\"] for attr in metadata[key]]\n",
        "print(\" Confirmed corrected length:\", len(flat_feature_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generalized Linear Regression Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------------------------------------+---------------------+-----------------------+---------------------+------------------------+\n",
            "|   |                      Feature                      |      Estimate       |       Std Error       |       t-stat        |        P-Value         |\n",
            "+---+---------------------------------------------------+---------------------+-----------------------+---------------------+------------------------+\n",
            "| 0 |                     SALARY_TO                     | 0.5031302008815193  | 0.0004013639288181846 | 1253.5511159734392  |          0.0           |\n",
            "| 1 |                    SALARY_FROM                    | 0.4896844366974874  | 0.000546766800642098  |  895.6001646815869  |          0.0           |\n",
            "| 2 |               MIN_YEARS_EXPERIENCE                | 3.7203359183413447  |   4.95761428938598    | 0.7504286741924255  |  0.45301683756798905   |\n",
            "| 3 |  EMPLOYMENT_TYPE_NAME_vec_Full-time (> 32 hours)  | -411.78801112405444 |  116.92931503205385   | -3.5216832580535593 | 0.00043101091416386517 |\n",
            "| 4 | EMPLOYMENT_TYPE_NAME_vec_Part-time (â‰¤ 32 hours) | -383.97695070473736 |  140.87699207997346   |  -2.72561860553326  |  0.006430937967740968  |\n",
            "+---+---------------------------------------------------+---------------------+-----------------------+---------------------+------------------------+\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "df_summary = pd.DataFrame({\n",
        "    \"Feature\": flat_feature_names,\n",
        "    \"Coefficient\": list(lr_model.coefficients),\n",
        "    \"Std Error\": list(summary.coefficientStandardErrors)[:len(flat_feature_names)],\n",
        "    \"T-Value\": list(summary.tValues)[:len(flat_feature_names)],\n",
        "    \"P-Value\": list(summary.pValues)[:len(flat_feature_names)]\n",
        "})\n",
        "\n",
        "z = 1.96  # 95% CI\n",
        "df_summary[\"CI Lower\"] = df_summary[\"Coefficient\"] - z * df_summary[\"Std Error\"]\n",
        "df_summary[\"CI Upper\"] = df_summary[\"Coefficient\"] + z * df_summary[\"Std Error\"]\n",
        "\n",
        "\n",
        "df_summary\n",
        "print(tabulate(coef_table, headers=\"keys\", tablefmt=\"pretty\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **SALARY_TO and SALARY_FROM:** Highly significant (p ≈ 0), strong positive impact.\n",
        "2. **MIN_YEARS_EXPERIENCE:** Not significant (p = 0.45), wide confidence interval.\n",
        "3. **Categorical vars:** Both have negative, significant coefficients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diagnostic Plot\n",
        "\n",
        "Diagnostic plots are essential for evaluating the performance of regression models. In this section, we will create several diagnostic plots to assess the linear regression model's assumptions and performance. There are four (2*2 grid) main plots we will create, you can use `seaborn` or `matplotlib` for this:\n",
        "\n",
        "1. **Predicted vs Actual Plot**\n",
        "2. **Residuals vs Predicted Plot**\n",
        "3. **Histogram of Residuals**\n",
        "4. **QQ Plot of Residuals**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Load predictions from GLR model\n",
        "df_pred = summary.predictions.select()\n",
        "\n",
        "# Compute residuals\n",
        "df_pred[\"residuals\"] = \n",
        "df_pred[\"fitted\"] = \n",
        "\n",
        "# Standardized residuals\n",
        "res_mean = \n",
        "res_std = \n",
        "df_pred[\"std_residuals\"] = \n",
        "\n",
        "# Square root of standardized residuals (for Scale-Location)\n",
        "df_pred[\"sqrt_std_resid\"] = \n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Plot 1: Residuals vs Fitted\n",
        "plt.subplot(2, 2, 1)\n",
        "\n",
        "\n",
        "# Plot 2: Normal Q-Q\n",
        "plt.subplot(2, 2, 2)\n",
        "\n",
        "\n",
        "# Plot 3: Scale-Location\n",
        "plt.subplot(2, 2, 3)\n",
        "\n",
        "\n",
        "# Plot 4: Residuals vs Leverage — Approximate\n",
        "# Note: Leverage & Cook's Distance require X matrix; we approximate using fitted & residual\n",
        "plt.subplot(2, 2, 4)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"_output/glr_diagnostic_classic.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "The evaluation of the model is crucial to understand its performance. In this section, we will calculate and visualize the following metrics:\n",
        "1. **R² (Coefficient of Determination)**: Indicates how well the model explains the variance in the target variable.\n",
        "2. **RMSE (Root Mean Squared Error)**: Measures the average magnitude of the errors between predicted and actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, pow, sqrt, avg\n",
        "import numpy as np\n",
        "\n",
        "pred_glr = lr_model.transform(test_data)\n",
        "\n",
        "# R²\n",
        "r2_eval = \n",
        "r2 = \n",
        "# AIC from GLR summary\n",
        "aic = \n",
        "\n",
        "# BIC calculation\n",
        "n = \n",
        "k = \n",
        "rss = \n",
        "bic = \n",
        "\n",
        "# RMSE manually\n",
        "residuals_df = \n",
        "rmse = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation Plot\n",
        "\n",
        "- Display the predicted vs actual salary plot with a red line indicating the ideal fit (y=x).\n",
        "- Use `seaborn` or `matplotlib` to create the plot.\n",
        "- Customize the plot with appropriate titles, labels, and legends.\n",
        "- Describe the plot in a few sentences, highlighting key insights and observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert GLR predictions to pandas\n",
        "pandas_df = \n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "\n",
        "plt.title(f\"Predicted vs Actual Salary (GLR Model)\\n\"\n",
        "          f\"RMSE = {rmse:.2f} | R² = {r2:.4f} | AIC = {aic:.2f} | BIC = {bic:.2f}\", loc=\"left\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figure\n",
        "plt.savefig(\"_output/glr_predicted_vs_actual.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Submission {.unnumbered}\n",
        "1. Save figures in the `_output/` folder.\n",
        "2. Commit and push code and output files:\n",
        "```bash\n",
        "git add .\n",
        "git commit -m \"Add Lab 08 Salary Prediction models and output\"\n",
        "git push origin main\n",
        "```\n",
        "3. Submit your GitHub repository link.\n",
        "\n",
        "# Resources {.unnumbered}\n",
        "- [PySpark MLlib Docs](https://spark.apache.org/docs/latest/ml-guide.html)  \n",
        "- [Seaborn Docs](https://seaborn.pydata.org/)  \n",
        "- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
